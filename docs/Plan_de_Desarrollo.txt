# Plan de Desarrollo - Proyecto Lucy
## Asistente de IA de Escritorio Inteligente

Este documento define el objetivo, la arquitectura, las estrategias y las acciones para desarrollar Lucy con un flujo lógico y trazable: de los objetivos a acciones, entregables, pruebas y métricas. Todas las referencias de módulos usan la estructura `src/lucy/*`.

### Objetivo General
- Construir un asistente de IA local, extensible y confiable, con respuestas rápidas, capacidad de aprendizaje y observabilidad completa.

### Arquitectura y Componentes
- `src/lucy/lucy_ai.py`: motor de conversación, detección de intención y generación de respuesta.
- `src/lucy/database.py`: `ConversationDB` para sesiones, contexto y historial.
- `src/lucy/config_manager.py`: `ConfigManager` y `ConfigWatcher` para configuración dinámica.
- `src/lucy/logging_system.py`: logging estructurado, métricas y trazabilidad.
- `src/lucy/training.py`: pipeline de entrenamiento y evaluación de modelos.
- `src/lucy/utils.py`: utilidades (normalización de texto, saludos, etc.).

### Diagrama de Arquitectura (alto nivel)
```
[Usuario]
   │
   ▼
[LucyAI] ──► [ConfigManager]
   │             │
   │             └── Carga y aplica configuración (runtime)
   │
   ├─► [Intent Detection / Respuesta]
   │        │
   │        └── Usa modelos y reglas
   │
   ├─► [ConversationDB]
   │        └── Guarda/recupera contexto e historial
   │
   └─► [LoggingSystem]
            └── Logs, métricas y auditoría

[Training] ───► [Modelos en disco] ───► [LucyAI]
```

### Flujo End-to-End (conexión entre componentes)
1. Entrada del usuario → normalización → detección de idioma → intención.
2. Generación de respuesta → actualización de contexto → persistencia en `ConversationDB`.
3. Logging y métricas (latencia, confianza, idioma, intención, errores).
4. Configuración dinámica puede ajustar comportamiento (p. ej., límites de historial, idioma preferido).
5. Entrenamiento produce/actualiza modelos → Lucy los carga bajo demanda.

### Estrategia de Implementación
- Iterar por fases, asegurando en cada día: objetivo claro, acciones atómicas, entregables verificables, criterios de aceptación, métricas y dependencias explícitas.
- Conectar tareas con pruebas (`tests/`) y documentación (`docs/`), con puertas de calidad (coverage y lint).

---

### Fase 1: Optimización y Base Sólida (Días 1-7)

**Día 1: Reestructuración y Limpieza del Código**
- Objetivo: consolidar la estructura en `src/lucy` y sanear imports/errores.
- Estrategia: mover módulos, normalizar rutas y manejo de errores.
- Acciones:
  - Reorganizar carpetas a `src/lucy/*` y actualizar imports.
  - Normalizar separadores en `ConfigManager.get_path` y paths relativos.
  - Manejo de `FileNotFoundError` en `LucyAI` para modelos y datos.
- Entregables: código consistente y ejecutable; documentación de estructura.
- Criterios de aceptación: ejecución local sin errores; tests básicos pasan.
- Métricas: tiempo de arranque; conteo de errores en logs.
- Dependencias: ninguna.

**Día 2: Sistema de Logging y Monitoreo**
- Objetivo: observabilidad con logs estructurados y métricas.
- Estrategia: configurar `logging.yaml`, niveles, formatos y sumarios.
- Acciones:
  - Implementar `LoggingSystem` y hooks en `LucyAI` y `Database`.
  - Métricas: latencia, confidencia, intención, volumen por idioma.
  - Exportar `lucy.logging_system` para import claro.
- Entregables: panel básico de métricas en archivo y/o consola.
- Criterios de aceptación: logs presentes por ruta crítica; métricas visibles.
- Métricas: latencia promedio < 2s; errores críticos = 0.
- Dependencias: Día 1.

**Día 3: Mejora del Sistema de Entrenamiento**
- Objetivo: reproducibilidad y calidad de modelos.
- Estrategia: EarlyStopping, ReduceLROnPlateau, CSVLogger; validación de datos.
- Acciones:
  - `training.py`: pipeline con semillas y guardado de mejor modelo.
  - Validación de `data/intents/*.json` y split train/val.
  - Métricas de evaluación (accuracy, F1) y reporte.
- Entregables: modelo entrenado y artefactos de evaluación.
- Criterios de aceptación: métricas mínimas; carga del modelo desde `LucyAI`.
- Métricas: F1 > 0.80 en intents base.
- Dependencias: Día 2.

**Día 4: Base de Datos de Conversaciones**
- Objetivo: persistencia y contexto estable.
- Estrategia: `ConversationDB` con sesiones, límites de historial y expiración de contexto.
- Acciones:
  - Implementar `save_conversation(...)` y recuperación de historial.
  - API de contexto: `set_context`, `get_session_context`.
  - Índices y consultas de búsqueda básicas.
- Entregables: historial consultable por sesión; contexto operativo.
- Criterios de aceptación: pruebas en `tests/test_lucy_ai.py` pasan.
- Métricas: tiempo de escritura/lectura; tamaño medio de sesión.
- Dependencias: Día 1.

**Día 5: Sistema de Configuración Dinámico**
- Objetivo: ajustar comportamiento en tiempo real.
- Estrategia: `ConfigWatcher` sobre `config/config.json` con observadores.
- Acciones:
  - Hot-reload de parámetros (p. ej., idioma, límites, respuestas).
  - Observers conectados en `LucyAI`, `LoggingSystem`, `Database`.
- Entregables: cambio de config sin reinicio.
- Criterios de aceptación: modificación de JSON refleja cambios inmediatos.
- Métricas: latencia de propagación de cambios < 500 ms.
- Dependencias: Días 1-2.

**Día 6: Testing y Calidad de Código**
- Objetivo: asegurar estabilidad con pruebas y cobertura, con arquitectura de pruebas escalable y mantenible.
- Alcance: pruebas unitarias e integración para `LucyAI`, `ConversationDB`, `ConfigManager`, `LoggingSystem` y `Training`; cobertura y pipeline local de CI.

1) Dependencias técnicas y requisitos por funcionalidad
- `LucyAI` (src/lucy/lucy_ai.py): depende de `ConfigManager`, `ConversationDB`, `LoggingSystem`, datos `data/intents/*` y modelos en disco. Requisitos: fixtures para datos/modelos, semillas determinísticas, mock de `LoggingSystem` y/o captura con `caplog`.
- `ConversationDB` (src/lucy/database.py): usa `sqlite3`; requiere DB temporal vía `tmp_path`, uso de timestamps en formato string (`YYYY-MM-DD HH:MM:SS`), atomicidad de transacciones y pruebas de concurrencia ligera.
- `ConfigManager` (src/lucy/config_manager.py): lee `config/config.json` y watchers; requiere archivo de config de prueba aislado, simulación de cambios en disco y verificación de propagación sin condiciones de carrera.
- `LoggingSystem` (src/lucy/logging_system.py): formatea JSON con `pythonjsonlogger.json.JsonFormatter`; requiere validación de estructura de log, niveles y ausencia de datos sensibles.
- `Training` (src/lucy/training.py): depende de `data/intents/*.json`; requiere dataset sintético de prueba, semilla fija, tiempo de ejecución acotado y verificación de guardado/carga de modelo.

2) Arquitectura óptima de pruebas
- Framework: `pytest` + `pytest-cov` (cobertura). Comando: `pytest -q --cov=src/lucy --cov-report=term-missing`.
- Estructura de módulos de prueba en `tests/`:
  - `test_lucy_ai.py`, `test_database.py`, `test_config_manager.py`, `test_logging.py`, `test_training.py`.
- Fixtures reutilizables en `tests/conftest.py`:
  - `temp_db_path(tmp_path)` y `conversation_db(temp_db_path)` para DB aislada.
  - `test_config(tmp_path)` y `config_manager(test_config)` para override de configuración.
  - `dummy_model_path(tmp_path)` y `lucy_ai(config_manager, conversation_db, dummy_model_path)` para orquestar el motor con dependencias.
  - `disable_json_logging(monkeypatch)` para forzar formatter simple cuando convenga.
- Datos de prueba: `src/test_data` y mocks generados en tiempo de test.

3) Módulos y componentes necesarios
- Pruebas por componente:
  - LucyAI: intents, lenguaje, respuestas y latencia.
  - ConversationDB: persistencia, historial, métricas y limpieza.
  - ConfigManager: carga, reload y propagación de cambios.
  - LoggingSystem: formato JSON, niveles y trazabilidad.
  - Training: pipeline completo, guardado y carga del modelo.
- Utilidades de test: helpers para construir intents válidos y sesiones.

4) Flujos de datos y procesos clave (tests e2e)
- LucyAI: input → normalización → intención → respuesta → `save_conversation` → log → recuperar historial → aserciones.
- DB: inserción/consulta → `cleanup_old_data` → `get_metrics_summary` → aserciones de tipos y conteos.
- Config: escribir nuevo JSON → watcher detecta → componentes ajustan comportamiento → validar latencia de propagación.
- Logging: emitir registros → capturar con `caplog` → parsear JSON → aserciones de campos.
- Training: cargar dataset → entrenar → guardar modelo → cargar en `LucyAI` → validar predicciones.

5) Criterios de validación y pruebas unitarias
- LucyAI: respuesta no vacía, intención válida, idioma detectado, fallback ante intent desconocido, p95 latencia local < 2s.
- ConversationDB: `save_conversation` persiste; `get_session_history` retorna último N; `get_metrics_summary` entrega dicts con campos esperados; `cleanup_old_data` elimina registros antiguos.
- ConfigManager: carga por defecto; reload en cambio de archivo; manejo de JSON inválido con log de error; observadores notificados.
- LoggingSystem: registros en JSON con campos `timestamp`, `session_id`, `intent`, `confidence`; niveles correctos; sin datos sensibles; sin deprecations de import.
- Training: métricas mínimas calculadas (accuracy/F1 sobre dataset sintético); guardado de mejor modelo; carga desde disco; errores manejados ante dataset vacío o inválido.

6) Acciones detalladas
- Ampliar `tests/` con casos por componente y parametrizaciones.
- Añadir `tests/conftest.py` con fixtures descritas y datos sintéticos.
- Configurar cobertura y reporte; umbral mínimo 85% (camino a 90%).
- Gating local: script o tarea que ejecute `pytest -q --cov=src/lucy --disable-warnings`.

7) Casos límite y manejo de errores
- Config ausente o inválido: fallback a valores por defecto y log de error.
- DB bloqueada o ruta no escribible: reintentos limitados, mensaje de error y abortar operación con estado claro.
- JSON de intents corrupto: validación previa, excepción controlada y reporte.
- Entrenamiento sin datos: terminar temprano con mensaje y no generar modelo.
- Watchers de config: evitar condiciones de carrera con bloqueos ligeros.

8) Documentación técnica
- Actualizar `docs/` con guía de ejecución de pruebas y cobertura.
- Comandos recomendados:
  - `pytest -q`
  - `pytest -q --cov=src/lucy --cov-report=term-missing`
  - `pytest -q -k "lucy_ai or database"`
- Incluir matriz de casos y trazabilidad a objetivos de calidad.

9) Ejemplos de código (referencia)
```python
# tests/conftest.py
import json
import os
import pytest
from src.lucy.database import ConversationDB
from src.lucy.config_manager import ConfigManager
from src.lucy.lucy_ai import LucyAI

@pytest.fixture
def temp_db_path(tmp_path):
    return tmp_path / "test.db"

@pytest.fixture
def conversation_db(temp_db_path):
    db = ConversationDB(str(temp_db_path))
    yield db

@pytest.fixture
def test_config(tmp_path):
    cfg = {"language": "es", "history_limit": 10}
    p = tmp_path / "config.json"
    p.write_text(json.dumps(cfg), encoding="utf-8")
    return str(p)

@pytest.fixture
def config_manager(test_config):
    return ConfigManager(config_path=test_config)

@pytest.fixture
def lucy_ai(config_manager, conversation_db):
    return LucyAI(config=config_manager, db=conversation_db)
```

```python
# tests/test_logging.py
import json
import logging
from src.lucy.logging_system import LoggingSystem

def test_json_logging_structure(caplog):
    ls = LoggingSystem({"json_logging": True})
    logger = ls.get_logger("test")
    with caplog.at_level(logging.INFO):
        logger.info("hola", extra={"session_id": "s1", "intent": "saludo", "confidence": 0.9})
    record = caplog.records[-1]
    payload = json.loads(record.message)
    assert set(["timestamp","session_id","intent","confidence"]).issubset(payload.keys())
```

- Entregables: suite de pruebas, cobertura ≥ 85%, pipeline local de CI y documentación actualizada.
- Criterios de aceptación: `pytest -q` sin fallos; cobertura >= 85%; sin `DeprecationWarning` en rutas críticas.
- Métricas: cobertura por módulo, tasa de fallos por commit y latencia media en `LucyAI`.
- Dependencias: Días 1-5.

**Día 7: Documentación y Deployment**
- Objetivo: facilitar instalación, uso y mantenimiento.
- Estrategia: README, guías en `docs/`, scripts de arranque.
- Acciones:
  - Actualizar docs con `src/lucy/*` y ejemplos reales.
  - Scripts `run_lucy.*` y guía de troubleshooting.
- Entregables: documentación completa y scripts funcionales.
- Criterios de aceptación: instalación y ejecución guiadas sin bloqueos.
- Métricas: tiempo de onboarding < 30 min.
- Dependencias: Días 1-6.

---

### Fase 2: Funcionalidades Core (Días 8-14)

Para cada día se aplicará el mismo esquema (Objetivo, Estrategia, Acciones, Entregables, Criterios, Métricas, Dependencias). A continuación, se detallan los puntos críticos de implementación.

**Día 8: Sistema de Plugins**
- Estrategia: cargador dinámico con registro y contratos claros (interfaces).
- Acciones: módulo `plugins/` con descubrimiento y sandbox básico; ejemplo funcional.
- Dependencias: Config estable (Día 5) y Logging (Día 2).

**Día 9: Integración con APIs Externas**
- Estrategia: clientes HTTP con autenticación, rate limiting y cache.
- Acciones: capa `services/` y pruebas contra sandbox/mock.

**Día 10: PLN Avanzado**
- Estrategia: pipeline con spaCy/Transformers; normalización y scoring.
- Acciones: nuevas funciones en `lucy_ai.py` y modelos en `src/test_models/`.

**Día 11: Memoria a Largo Plazo**
- Estrategia: almacenamiento vectorial y búsqueda semántica.
- Acciones: módulo `memory/` y conexión con `ConversationDB`.

**Día 12: Interface Web Básica**
- Estrategia: API REST (FastAPI) y frontend mínimo.
- Acciones: endpoints para chat/contexto; WebSocket para tiempo real.

**Día 13: Comandos de Sistema**
- Estrategia: capa segura con whitelist y auditoría.
- Acciones: integración con `LoggingSystem` y prompts claros.

**Día 14: Notificaciones**
- Estrategia: programador de tareas y conectores de calendario.
- Acciones: colas ligeras y persistencia de recordatorios.

---

### Fase 3: Inteligencia Avanzada (Días 15-21)
- Enfoque incremental con validaciones fuertes; cada subcomponente integra logging y pruebas.
- Prioridades: datos (Día 15), generación (Día 16), voz (Día 17), visión (Día 18), cloud (Día 19), recomendaciones (Día 20), automatización (Día 21).

---

### Fase 4: Integración y Distribución (Días 22-30)
- Cerrar APIs, GUI desktop, web completa, seguridad y QA; culminar en release y plan de mantenimiento.

---

### Mapa de Trazabilidad (Objetivos → Acciones → Pruebas → Métricas)
- Objetivo: "Respuestas rápidas" → Acciones: mejoras en LucyAI, cache → Pruebas: latencia y carga → Métricas: p95 < 2s.
- Objetivo: "Confiabilidad" → Acciones: cobertura de pruebas → Pruebas: unitarias/integración → Métricas: cobertura ≥ 90% (meta).
- Objetivo: "Aprendizaje" → Acciones: training reproducible → Pruebas: evaluación de modelo → Métricas: F1 ≥ 0.85.

### Calidad, Testing y CI
- Pruebas: `pytest -q`; integración para `LucyAI`, `Database`, `ConfigManager`, `Training`.
- Cobertura: umbral mínimo 85% (subida progresiva a 90%).
- CI local: scripts en `run_lucy.*` y verificación previa a release.

### Seguridad y Privacidad
- Logs sin datos sensibles; cifrado en repositorios persistentes.
- Autenticación para APIs; auditoría de accesos.

### Riesgos y Mitigación
- Datos inconsistentes → validadores y pruebas de datos.
- Rendimiento insuficiente → profiling y cache.
- Dependencias externas → mock y aislamiento.

### Tecnologías por Implementar (mapeo)
- Backend: FastAPI (Fase 2 Día 12), SQLAlchemy (DB avanzada), Redis (cache), Celery (tareas async).
- Frontend: React/Vue (Fase 4 Día 24), WebSocket (Día 12), PWA.
- IA/ML: Transformers, spaCy, scikit-learn; OpenCV (Fase 3 Día 18).
- Infra: Docker, GitHub Actions, AWS/GCP, Prometheus.

### Métricas de Éxito
- Tiempo de respuesta < 2 segundos (p95 en escenarios comunes).
- Precisión de respuestas > 85% (PLN base; objetivo 90% en avance).
- Cobertura de pruebas > 90% (meta; umbral operativo 85%).
- Documentación completa al 100% (actualizada por cambios de código).
- 0 vulnerabilidades críticas (escaneo regular).
- Soporte para 10+ idiomas (detección y respuestas consistentes).

### Diagrama de Flujo Operativo (resumen)
```
Usuario → LucyAI → Detección de intención → Generación de respuesta
          │              │                          │
          │              └── usa modelos y reglas   └── texto final
          ├─► ConversationDB (historial y contexto)
          ├─► LoggingSystem (métricas y auditoría)
          └─► ConfigManager (ajustes en runtime)
```

### Notas de Interconexión
- Cada funcionalidad nueva debe registrar métricas y respetar configuración dinámica.
- Cambios en modelos implican actualización de `LucyAI` y validación en `tests/`.
- DB y Config son dependencias transversales; cualquier cambio requiere verificación de compatibilidad.